"""
í†µí•© ìˆ˜ì˜ í¬ë¡¤ë§ ì„œë¹„ìŠ¤
1. ê¸°ë³¸ ìŠ¤ì¼€ì¤„ (ì´ìš©ì•ˆë‚´ í˜ì´ì§€) í¬ë¡¤ë§
2. ì›”ë³„ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§
3. ë°ì´í„° ë³‘í•© ë° ì €ì¥

ì§€ì› ê¸°ê´€:
- ì„±ë‚¨ì‹œì²­ì†Œë…„ì²­ë…„ì¬ë‹¨ (snyouth): 3ê°œ ìœ ìŠ¤ì„¼í„°
- ì„±ë‚¨ë„ì‹œê°œë°œê³µì‚¬ (snhdc): 5ê°œ ì²´ìœ¡ì„¼í„°
"""
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional

# ì„±ë‚¨ì‹œì²­ì†Œë…„ì²­ë…„ì¬ë‹¨ í¬ë¡¤ëŸ¬
from crawler.snyouth.facility_info_crawler import FacilityInfoCrawler as SnyouthFacilityInfoCrawler
from crawler.snyouth.list_crawler import ListCrawler as SnyouthListCrawler
from crawler.snyouth.detail_crawler import DetailCrawler as SnyouthDetailCrawler

# ì„±ë‚¨ë„ì‹œê°œë°œê³µì‚¬ í¬ë¡¤ëŸ¬
from crawler.snhdc.facility_info_crawler import FacilityInfoCrawler as SnhdcFacilityInfoCrawler
from crawler.snhdc.list_crawler import ListCrawler as SnhdcListCrawler
from crawler.snhdc.detail_crawler import DetailCrawler as SnhdcDetailCrawler
from crawler.snhdc.attachment_downloader import AttachmentDownloader

# íŒŒì„œ
from parser.hwp_text_extractor import HwpTextExtractor
from parser.pdf_text_extractor import PdfTextExtractor
from parser.llm_parser import LLMParser
from parser.content_validator import ContentValidator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

STORAGE_DIR = Path(__file__).parent.parent / "storage"
DOWNLOAD_DIR = Path(__file__).parent.parent / "downloads"


class SwimCrawlerService:
    """í†µí•© ìˆ˜ì˜ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ (snyouth + snhdc)"""

    def __init__(self):
        # ì„±ë‚¨ì‹œì²­ì†Œë…„ì²­ë…„ì¬ë‹¨ í¬ë¡¤ëŸ¬
        self.snyouth_facility_crawler = SnyouthFacilityInfoCrawler()
        self.snyouth_list_crawler = SnyouthListCrawler()
        self.snyouth_detail_crawler = SnyouthDetailCrawler()

        # ì„±ë‚¨ë„ì‹œê°œë°œê³µì‚¬ í¬ë¡¤ëŸ¬
        self.snhdc_facility_crawler = SnhdcFacilityInfoCrawler()
        self.snhdc_list_crawler = SnhdcListCrawler()
        self.snhdc_detail_crawler = SnhdcDetailCrawler
        self.snhdc_downloader = AttachmentDownloader(download_dir=DOWNLOAD_DIR / "snhdc")

        # íŒŒì„œ
        self.hwp_extractor = HwpTextExtractor()
        self.pdf_extractor = PdfTextExtractor()
        self.llm_parser = LLMParser()
        self.validator = ContentValidator()

    def crawl_base_schedules(self, save: bool = True) -> Dict[str, List[Dict]]:
        """
        ê¸°ë³¸ ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ (ì´ìš©ì•ˆë‚´ í˜ì´ì§€)
        ì–‘ìª½ ê¸°ê´€ ëª¨ë‘ í¬ë¡¤ë§

        Args:
            save: JSON íŒŒì¼ë¡œ ì €ì¥ ì—¬ë¶€

        Returns:
            {"snyouth": [...], "snhdc": [...]} í˜•íƒœì˜ ì‹œì„¤ ê¸°ë³¸ ì •ë³´
        """
        logger.info("=== ê¸°ë³¸ ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ ì‹œì‘ ===")

        all_facilities = {
            "snyouth": [],
            "snhdc": []
        }

        # 1. ì„±ë‚¨ì‹œì²­ì†Œë…„ì²­ë…„ì¬ë‹¨ (snyouth)
        logger.info("ì„±ë‚¨ì‹œì²­ì†Œë…„ì²­ë…„ì¬ë‹¨ í¬ë¡¤ë§ ì¤‘...")
        snyouth_facilities = self.snyouth_facility_crawler.crawl_all_facilities()
        all_facilities["snyouth"] = [
            self.snyouth_facility_crawler.to_dict(f) for f in snyouth_facilities
        ]
        logger.info(f"  âœ“ {len(all_facilities['snyouth'])}ê°œ ì‹œì„¤ ì™„ë£Œ")

        # 2. ì„±ë‚¨ë„ì‹œê°œë°œê³µì‚¬ (snhdc)
        logger.info("ì„±ë‚¨ë„ì‹œê°œë°œê³µì‚¬ í¬ë¡¤ë§ ì¤‘...")
        snhdc_facilities = self.snhdc_facility_crawler.crawl_all_facilities()
        all_facilities["snhdc"] = [
            self.snhdc_facility_crawler.to_dict(f) for f in snhdc_facilities
        ]
        logger.info(f"  âœ“ {len(all_facilities['snhdc'])}ê°œ ì‹œì„¤ ì™„ë£Œ")

        if save:
            self._save_base_schedules(all_facilities)

        total = len(all_facilities["snyouth"]) + len(all_facilities["snhdc"])
        logger.info(f"ê¸°ë³¸ ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {total}ê°œ ì‹œì„¤")
        return all_facilities

    def crawl_monthly_notices(self, keyword: str = "ìˆ˜ì˜", max_pages: int = 5, save: bool = True) -> Dict[str, List[Dict]]:
        """
        ì›”ë³„ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§
        ì–‘ìª½ ê¸°ê´€ ëª¨ë‘ í¬ë¡¤ë§

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            save: JSON íŒŒì¼ë¡œ ì €ì¥ ì—¬ë¶€

        Returns:
            {"snyouth": [...], "snhdc": [...]} í˜•íƒœì˜ ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´
        """
        logger.info("=== ì›”ë³„ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ì‹œì‘ ===")

        all_notices = {
            "snyouth": [],
            "snhdc": []
        }

        # 1. ì„±ë‚¨ì‹œì²­ì†Œë…„ì²­ë…„ì¬ë‹¨ (snyouth)
        logger.info("ì„±ë‚¨ì‹œì²­ì†Œë…„ì²­ë…„ì¬ë‹¨ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ì¤‘...")
        snyouth_posts = self.snyouth_list_crawler.get_posts(keyword=keyword, max_pages=max_pages)
        logger.info(f"  ê²Œì‹œê¸€ ëª©ë¡: {len(snyouth_posts)}ê°œ")

        snyouth_details = []
        for post in snyouth_posts:
            detail = self.snyouth_detail_crawler.get_detail(post.detail_url)
            if detail:
                snyouth_details.append(detail)

        # to_dict ë©”ì„œë“œë¥¼ ì§ì ‘ í˜¸ì¶œ (snyouth DetailCrawlerì—ëŠ” to_dictê°€ ì—†ìœ¼ë¯€ë¡œ)
        all_notices["snyouth"] = [
            {
                "title": d.title,
                "date": d.date,
                "content_text": d.content,
                "source_url": d.source_url,
                "attachments": [{"filename": att.filename, "size": att.file_size, "url": att.download_url} for att in d.attachments],
                "has_attachment": len(d.attachments) > 0,
                "file_hwp": next((att.filename for att in d.attachments if att.file_ext == "hwp"), None),
                "file_pdf": next((att.filename for att in d.attachments if att.file_ext == "pdf"), None)
            }
            for d in snyouth_details
        ]
        logger.info(f"  âœ“ {len(all_notices['snyouth'])}ê°œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")

        # 2. ì„±ë‚¨ë„ì‹œê°œë°œê³µì‚¬ (snhdc)
        logger.info("ì„±ë‚¨ë„ì‹œê°œë°œê³µì‚¬ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ì¤‘...")

        # SNHDCëŠ” list APIê°€ ì´ë¯¸ contentë¥¼ í¬í•¨í•˜ë¯€ë¡œ listë§Œ ê°€ì ¸ì˜¤ê¸°
        snhdc_list_crawler = self.snhdc_list_crawler
        response = snhdc_list_crawler.session.post(
            "https://spo.isdc.co.kr/selectNoticeList.ajax",
            data={"searchWord": keyword, "page": 1, "perPageNum": max_pages * 10, "brd_flg": "1"},
            timeout=10
        )

        json_data = response.json()
        notice_list = json_data.get("data", [])
        logger.info(f"  ê²Œì‹œê¸€ ëª©ë¡: {len(notice_list)}ê°œ")

        # DetailCrawlerë¡œ ë³€í™˜
        snhdc_details = []
        for item in notice_list:
            detail = SnhdcDetailCrawler.from_list_item(item)
            if detail:
                snhdc_details.append(detail)

        all_notices["snhdc"] = [
            SnhdcDetailCrawler.to_dict(d) for d in snhdc_details
        ]
        logger.info(f"  âœ“ {len(all_notices['snhdc'])}ê°œ ìƒì„¸ ì •ë³´ ë³€í™˜ ì™„ë£Œ")

        if save:
            self._save_monthly_notices(all_notices)

        total = len(all_notices["snyouth"]) + len(all_notices["snhdc"])
        logger.info(f"ì›”ë³„ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {total}ê°œ")
        return all_notices

    def parse_snhdc_attachments(self, monthly_notices: Optional[Dict[str, List[Dict]]] = None,
                                 save: bool = True, max_files: int = 10) -> List[Dict]:
        """
        SNHDC ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±

        Args:
            monthly_notices: ì›”ë³„ ê³µì§€ì‚¬í•­ (ì—†ìœ¼ë©´ íŒŒì¼ì—ì„œ ë¡œë“œ)
            save: íŒŒì‹± ê²°ê³¼ ì €ì¥ ì—¬ë¶€
            max_files: ìµœëŒ€ ì²˜ë¦¬ íŒŒì¼ ìˆ˜

        Returns:
            íŒŒì‹±ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("=== SNHDC ì²¨ë¶€íŒŒì¼ íŒŒì‹± ì‹œì‘ ===")

        # ë°ì´í„° ë¡œë“œ
        if monthly_notices is None:
            monthly_notices = self._load_monthly_notices()

        snhdc_notices = monthly_notices.get("snhdc", [])

        # ì²¨ë¶€íŒŒì¼ì´ ìˆëŠ” ê³µì§€ë§Œ í•„í„°ë§
        notices_with_files = [n for n in snhdc_notices if n.get("has_attachment")]
        logger.info(f"ì²¨ë¶€íŒŒì¼ì´ ìˆëŠ” ê³µì§€: {len(notices_with_files)}ê°œ")

        if not notices_with_files:
            logger.warning("ì²¨ë¶€íŒŒì¼ì´ ìˆëŠ” ê³µì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        notices_to_process = notices_with_files[:max_files]
        logger.info(f"ì²˜ë¦¬í•  ê³µì§€: {len(notices_to_process)}ê°œ")

        # 1. ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        logger.info("ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        downloaded_files_info = []

        for notice in notices_to_process:
            # PostDetail ì¬êµ¬ì„±
            from dataclasses import dataclass
            from typing import Optional as Opt

            @dataclass
            class PostDetailForDownload:
                post_id: str
                title: str
                facility_name: str
                date: str
                content_html: str
                content_text: str
                file_hwp: Opt[str] = None
                file_pdf: Opt[str] = None
                has_attachment: bool = False

            post_detail = PostDetailForDownload(
                post_id=notice["post_id"],
                title=notice["title"],
                facility_name=notice["facility_name"],
                date=notice["date"],
                content_html=notice.get("content_html", ""),
                content_text=notice.get("content_text", ""),
                file_hwp=notice.get("file_hwp"),
                file_pdf=notice.get("file_pdf"),
                has_attachment=notice["has_attachment"]
            )

            # ë‹¤ìš´ë¡œë“œ
            downloaded = self.snhdc_downloader.download_from_post_detail(post_detail)
            for file_path in downloaded:
                file_ext = file_path.suffix.lower()
                if file_ext in [".hwp", ".pdf"]:
                    downloaded_files_info.append((notice, file_path, file_ext))

        logger.info(f"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(downloaded_files_info)}ê°œ íŒŒì¼")

        # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        logger.info("í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        extracted_texts = []

        for notice, file_path, file_ext in downloaded_files_info:
            try:
                if file_ext == ".hwp":
                    text = self.hwp_extractor.extract_text(file_path)
                elif file_ext == ".pdf":
                    text = self.pdf_extractor.extract_text(file_path)
                else:
                    continue

                if text and len(text) > 50:
                    extracted_texts.append((notice, text, file_path))
                    logger.info(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {file_path.name} ({len(text)}ì)")
            except Exception as e:
                logger.warning(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {file_path.name} - {e}")

        logger.info(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(extracted_texts)}ê°œ")

        # 3. ì½˜í…ì¸  ê²€ì¦
        logger.info("ì½˜í…ì¸  ê²€ì¦ ì¤‘...")
        validated_texts = []

        for notice, text, file_path in extracted_texts:
            is_valid = self.validator.contains_swim_info(text)
            if is_valid:
                validated_texts.append((notice, text, file_path))
                logger.info(f"ê²€ì¦ í†µê³¼: {file_path.name}")

        logger.info(f"ê²€ì¦ í†µê³¼: {len(validated_texts)}ê°œ")

        # 4. LLM íŒŒì‹±
        logger.info("LLM íŒŒì‹± ì¤‘...")
        parsed_results = []

        for notice, text, file_path in validated_texts:
            result = self.llm_parser.parse(
                raw_text=text,
                facility_name=notice["facility_name"],
                notice_date=notice["date"],
                source_url=f"https://spo.isdc.co.kr (ì²¨ë¶€íŒŒì¼: {file_path.name})"
            )

            if result:
                result["source_file"] = file_path.name
                result["source_notice_title"] = notice["title"]
                result["source_notice_date"] = notice["date"]
                parsed_results.append(result)
                logger.info(f"íŒŒì‹± ì„±ê³µ: {file_path.name}")

        logger.info(f"LLM íŒŒì‹± ì™„ë£Œ: {len(parsed_results)}ê°œ")

        # 5. ê²°ê³¼ ì €ì¥
        if save and parsed_results:
            self._save_parsed_attachments(parsed_results)

        return parsed_results

    def _save_parsed_attachments(self, data: List[Dict]):
        """íŒŒì‹±ëœ ì²¨ë¶€íŒŒì¼ ë°ì´í„° ì €ì¥"""
        output = {
            "meta": {
                "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "source": "SNHDC attachments (HWP/PDF)",
                "parsed_count": len(data)
            },
            "parsed_schedules": data
        }

        file_path = STORAGE_DIR / "snhdc_parsed_attachments.json"
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        logger.info(f"íŒŒì‹± ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {file_path}")

    def merge_schedules(self, base_schedules: Optional[Dict[str, List[Dict]]] = None,
                       monthly_notices: Optional[Dict[str, List[Dict]]] = None) -> Dict:
        """
        ê¸°ë³¸ ìŠ¤ì¼€ì¤„ê³¼ ì›”ë³„ ê³µì§€ì‚¬í•­ ë³‘í•©

        ì „ëµ:
        1. ê¸°ë³¸ ìŠ¤ì¼€ì¤„ì„ baselineìœ¼ë¡œ ì‚¬ìš©
        2. ì›”ë³„ ê³µì§€ì‚¬í•­ì—ì„œ ì„ì‹œ ë³€ê²½ì‚¬í•­ ì¶”ì¶œ
        3. íŠ¹ì • ë‚ ì§œ/ê¸°ê°„ì— ëŒ€í•œ override ì •ë³´ ì œê³µ

        Args:
            base_schedules: {"snyouth": [...], "snhdc": [...]} í˜•íƒœ (ì—†ìœ¼ë©´ íŒŒì¼ì—ì„œ ë¡œë“œ)
            monthly_notices: {"snyouth": [...], "snhdc": [...]} í˜•íƒœ (ì—†ìœ¼ë©´ íŒŒì¼ì—ì„œ ë¡œë“œ)

        Returns:
            ë³‘í•©ëœ ìŠ¤ì¼€ì¤„ ì •ë³´
        """
        logger.info("=== ìŠ¤ì¼€ì¤„ ë³‘í•© ì‹œì‘ ===")

        # ë°ì´í„° ë¡œë“œ
        if base_schedules is None:
            base_schedules = self._load_base_schedules()

        if monthly_notices is None:
            monthly_notices = self._load_monthly_notices()

        # í†µê³„
        snyouth_base_count = len(base_schedules.get("snyouth", []))
        snhdc_base_count = len(base_schedules.get("snhdc", []))
        snyouth_notice_count = len(monthly_notices.get("snyouth", []))
        snhdc_notice_count = len(monthly_notices.get("snhdc", []))

        # ì‹œì„¤ë³„ë¡œ ê·¸ë£¹í™”
        merged = {
            "meta": {
                "merged_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "organizations": {
                    "snyouth": {
                        "name": "ì„±ë‚¨ì‹œì²­ì†Œë…„ì²­ë…„ì¬ë‹¨",
                        "base_schedule_count": snyouth_base_count,
                        "monthly_notice_count": snyouth_notice_count
                    },
                    "snhdc": {
                        "name": "ì„±ë‚¨ë„ì‹œê°œë°œê³µì‚¬",
                        "base_schedule_count": snhdc_base_count,
                        "monthly_notice_count": snhdc_notice_count
                    }
                }
            },
            "facilities": {}
        }

        # 1. ê¸°ë³¸ ìŠ¤ì¼€ì¤„ì„ baselineìœ¼ë¡œ ì„¤ì • (ì–‘ìª½ ê¸°ê´€ ëª¨ë‘)
        for org_key in ["snyouth", "snhdc"]:
            org_name = merged["meta"]["organizations"][org_key]["name"]
            for facility in base_schedules.get(org_key, []):
                facility_name = facility.get("facility_name")
                merged["facilities"][facility_name] = {
                    "organization": org_name,
                    "org_key": org_key,
                    "base_schedule": facility,
                    "monthly_updates": [],
                    "current_schedule": facility.copy()  # í˜„ì¬ ì ìš©ì¤‘ì¸ ìŠ¤ì¼€ì¤„
                }

        # 2. ì›”ë³„ ê³µì§€ì‚¬í•­ ì¶”ê°€ (ì–‘ìª½ ê¸°ê´€ ëª¨ë‘)
        for org_key in ["snyouth", "snhdc"]:
            for notice in monthly_notices.get(org_key, []):
                # ì‹œì„¤ëª… ì¶”ì¶œ (ë‹¤ì–‘í•œ í•„ë“œëª… ì§€ì›)
                facility_name = (
                    notice.get("facility_name") or
                    notice.get("pool_name") or
                    ""
                )

                # ì‹œì„¤ëª… ë§¤ì¹­ (ìœ ì—°í•œ ë§¤ì¹­)
                matched_facility = self._match_facility_name(facility_name, merged["facilities"].keys())

                if matched_facility:
                    merged["facilities"][matched_facility]["monthly_updates"].append({
                        "title": notice.get("title", ""),
                        "valid_month": notice.get("valid_month", ""),
                        "date": notice.get("date", ""),
                        "source_url": notice.get("source_url", ""),
                        "content_text_preview": notice.get("content_text", "")[:200] + "..." if notice.get("content_text") else "",
                        "has_attachment": notice.get("has_attachment", False),
                        "file_hwp": notice.get("file_hwp"),
                        "file_pdf": notice.get("file_pdf")
                    })

        logger.info("ìŠ¤ì¼€ì¤„ ë³‘í•© ì™„ë£Œ")
        return merged

    def _match_facility_name(self, name: str, candidates: List[str]) -> Optional[str]:
        """ì‹œì„¤ëª… ë§¤ì¹­ (ìœ ì—°í•œ ë§¤ì¹­)"""
        # ì •í™•íˆ ì¼ì¹˜
        if name in candidates:
            return name

        # ë¶€ë¶„ ì¼ì¹˜
        for candidate in candidates:
            if name in candidate or candidate in name:
                return candidate

        # í‚¤ì›Œë“œ ë§¤ì¹­
        keywords_map = {
            "ì¤‘ì›": "ì¤‘ì›ìœ ìŠ¤ì„¼í„°",
            "íŒêµ": "íŒêµìœ ìŠ¤ì„¼í„°",
            "ì•¼íƒ‘": "ì•¼íƒ‘ìœ ìŠ¤ì„¼í„°"
        }

        for keyword, full_name in keywords_map.items():
            if keyword in name and full_name in candidates:
                return full_name

        return None

    def _save_base_schedules(self, data: Dict[str, List[Dict]]):
        """ê¸°ë³¸ ìŠ¤ì¼€ì¤„ ì €ì¥"""
        snyouth_count = len(data.get("snyouth", []))
        snhdc_count = len(data.get("snhdc", []))

        output = {
            "meta": {
                "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "version": "2.0",
                "source": "official usage guide pages",
                "organizations": {
                    "snyouth": {
                        "name": "ì„±ë‚¨ì‹œì²­ì†Œë…„ì²­ë…„ì¬ë‹¨",
                        "facility_count": snyouth_count
                    },
                    "snhdc": {
                        "name": "ì„±ë‚¨ë„ì‹œê°œë°œê³µì‚¬",
                        "facility_count": snhdc_count
                    }
                },
                "total_facility_count": snyouth_count + snhdc_count
            },
            "snyouth": data.get("snyouth", []),
            "snhdc": data.get("snhdc", [])
        }

        file_path = STORAGE_DIR / "facility_base_schedules.json"
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        logger.info(f"ê¸°ë³¸ ìŠ¤ì¼€ì¤„ ì €ì¥ ì™„ë£Œ: {file_path}")

    def _save_monthly_notices(self, data: Dict[str, List[Dict]]):
        """ì›”ë³„ ê³µì§€ì‚¬í•­ ì €ì¥"""
        snyouth_count = len(data.get("snyouth", []))
        snhdc_count = len(data.get("snhdc", []))

        output = {
            "meta": {
                "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "version": "2.0",
                "organizations": {
                    "snyouth": {
                        "name": "ì„±ë‚¨ì‹œì²­ì†Œë…„ì²­ë…„ì¬ë‹¨",
                        "notice_count": snyouth_count
                    },
                    "snhdc": {
                        "name": "ì„±ë‚¨ë„ì‹œê°œë°œê³µì‚¬",
                        "notice_count": snhdc_count
                    }
                },
                "total_notice_count": snyouth_count + snhdc_count
            },
            "snyouth": data.get("snyouth", []),
            "snhdc": data.get("snhdc", [])
        }

        file_path = STORAGE_DIR / "swim_programs.json"
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        logger.info(f"ì›”ë³„ ê³µì§€ì‚¬í•­ ì €ì¥ ì™„ë£Œ: {file_path}")

    def _load_base_schedules(self) -> Dict[str, List[Dict]]:
        """ê¸°ë³¸ ìŠ¤ì¼€ì¤„ ë¡œë“œ"""
        file_path = STORAGE_DIR / "facility_base_schedules.json"
        if not file_path.exists():
            logger.warning(f"ê¸°ë³¸ ìŠ¤ì¼€ì¤„ íŒŒì¼ ì—†ìŒ: {file_path}")
            return {"snyouth": [], "snhdc": []}

        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            # v2.0 í˜•ì‹
            if "snyouth" in data and "snhdc" in data:
                return {"snyouth": data.get("snyouth", []), "snhdc": data.get("snhdc", [])}
            # v1.0 í˜•ì‹ (í•˜ìœ„ í˜¸í™˜)
            elif "facilities" in data:
                return {"snyouth": data.get("facilities", []), "snhdc": []}
            else:
                return {"snyouth": [], "snhdc": []}

    def _load_monthly_notices(self) -> Dict[str, List[Dict]]:
        """ì›”ë³„ ê³µì§€ì‚¬í•­ ë¡œë“œ"""
        file_path = STORAGE_DIR / "swim_programs.json"
        if not file_path.exists():
            logger.warning(f"ì›”ë³„ ê³µì§€ì‚¬í•­ íŒŒì¼ ì—†ìŒ: {file_path}")
            return {"snyouth": [], "snhdc": []}

        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            # v2.0 í˜•ì‹
            if "snyouth" in data and "snhdc" in data:
                return {"snyouth": data.get("snyouth", []), "snhdc": data.get("snhdc", [])}
            # v1.0 í˜•ì‹ (í•˜ìœ„ í˜¸í™˜) - ë¦¬ìŠ¤íŠ¸ í˜•íƒœ
            elif isinstance(data, list):
                return {"snyouth": data, "snhdc": []}
            else:
                return {"snyouth": [], "snhdc": []}


# í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ
if __name__ == "__main__":
    service = SwimCrawlerService()

    # 1. ê¸°ë³¸ ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§
    print("\n" + "="*60)
    print("1. ê¸°ë³¸ ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ (snyouth + snhdc)")
    print("="*60)
    base_schedules = service.crawl_base_schedules(save=True)
    snyouth_count = len(base_schedules.get("snyouth", []))
    snhdc_count = len(base_schedules.get("snhdc", []))
    print(f"  ì„±ë‚¨ì‹œì²­ì†Œë…„ì²­ë…„ì¬ë‹¨: {snyouth_count}ê°œ")
    print(f"  ì„±ë‚¨ë„ì‹œê°œë°œê³µì‚¬: {snhdc_count}ê°œ")
    print(f"  ì´ {snyouth_count + snhdc_count}ê°œ ì‹œì„¤")

    # 2. ì›”ë³„ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§
    print("\n" + "="*60)
    print("2. ì›”ë³„ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ (snyouth + snhdc)")
    print("="*60)
    monthly_notices = service.crawl_monthly_notices(keyword="ìˆ˜ì˜", max_pages=2, save=True)
    snyouth_notice_count = len(monthly_notices.get("snyouth", []))
    snhdc_notice_count = len(monthly_notices.get("snhdc", []))
    print(f"  ì„±ë‚¨ì‹œì²­ì†Œë…„ì²­ë…„ì¬ë‹¨: {snyouth_notice_count}ê°œ")
    print(f"  ì„±ë‚¨ë„ì‹œê°œë°œê³µì‚¬: {snhdc_notice_count}ê°œ")
    print(f"  ì´ {snyouth_notice_count + snhdc_notice_count}ê°œ ê³µì§€")

    # 3. SNHDC ì²¨ë¶€íŒŒì¼ íŒŒì‹±
    print("\n" + "="*60)
    print("3. SNHDC ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±")
    print("="*60)
    parsed_attachments = service.parse_snhdc_attachments(monthly_notices, save=True, max_files=5)
    print(f"  íŒŒì‹± ì™„ë£Œ: {len(parsed_attachments)}ê°œ")
    for result in parsed_attachments[:3]:  # ì²˜ìŒ 3ê°œë§Œ
        print(f"    - [{result['facility_name']}] {result.get('valid_month', 'N/A')}")
        print(f"      íŒŒì¼: {result['source_file']}")
        print(f"      ìŠ¤ì¼€ì¤„: {len(result.get('schedules', []))}ê°œ")

    # 4. ë°ì´í„° ë³‘í•©
    print("\n" + "="*60)
    print("4. ìŠ¤ì¼€ì¤„ ë³‘í•©")
    print("="*60)
    merged = service.merge_schedules(base_schedules, monthly_notices)
    print(f"ë³‘í•© ì™„ë£Œ: {len(merged['facilities'])}ê°œ ì‹œì„¤")

    # ë³‘í•© ê²°ê³¼ ì¶œë ¥ (ì¡°ì§ë³„ë¡œ)
    for org_key in ["snyouth", "snhdc"]:
        org_facilities = {
            name: data for name, data in merged["facilities"].items()
            if data.get("org_key") == org_key
        }

        if org_facilities:
            org_name = merged["meta"]["organizations"][org_key]["name"]
            print(f"\n{'='*60}")
            print(f"{org_name} ({len(org_facilities)}ê°œ ì‹œì„¤)")
            print(f"{'='*60}")

            for facility_name, facility_data in list(org_facilities.items())[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                print(f"\n  [{facility_name}]")
                weekday_schedule = facility_data['base_schedule'].get('weekday_schedule', [])
                print(f"    í‰ì¼ ìŠ¤ì¼€ì¤„: {len(weekday_schedule)}ê°œ")
                print(f"    ì›”ë³„ ì—…ë°ì´íŠ¸: {len(facility_data['monthly_updates'])}ê°œ")

                if facility_data['monthly_updates']:
                    print("    ìµœê·¼ ê³µì§€:")
                    for update in facility_data['monthly_updates'][:2]:  # ì²˜ìŒ 2ê°œë§Œ
                        title = update.get('title', '')
                        date = update.get('date', '')
                        has_file = update.get('has_attachment', False)
                        file_marker = " [ğŸ“]" if has_file else ""
                        print(f"      - [{date}] {title}{file_marker}")

    # ë³‘í•© ê²°ê³¼ ì €ì¥
    merged_file = STORAGE_DIR / "merged_schedules.json"
    with open(merged_file, "w", encoding="utf-8") as f:
        json.dump(merged, f, ensure_ascii=False, indent=2)
    print(f"\n{'='*60}")
    print(f"ë³‘í•© ê²°ê³¼ ì €ì¥: {merged_file}")
    print(f"{'='*60}")
